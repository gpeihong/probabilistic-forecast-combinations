{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a344836-2055-4318-b7ca-62f093c5cadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03 Submodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7f433e-8aff-4fcb-8a4b-697e5b57f59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "weatherclimateED = pd.read_csv('weatherclimateED.csv', parse_dates = [0], dayfirst = True)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "import itertools\n",
    "import statsmodels.api as sm\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "ConvergenceWarning('ignore')\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "# warnings.simplefilter(\"ignore\")\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\" # this change is only for this time's session\n",
    "import time\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366d1977-f012-4005-9a58-dd6b010c27c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Epiweeks Module converts dates to CDC Epiweek format\n",
    "## Further documentation on https://pypi.org/project/epiweeks/\n",
    "from epiweeks import Week, Year\n",
    "from datetime import date\n",
    "def create_epiweek(date):\n",
    "    return Week.fromdate(date)\n",
    "def create_epiweekplot(epiweek):\n",
    "    epiweek = str(epiweek)\n",
    "    return F'Y{epiweek[:4]}W{epiweek[4:]}'\n",
    "def create_epiweek_fromstr(str):\n",
    "    return Week.fromstring(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0f4c3f-fbee-4b7f-8197-c3948ab6c63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This section creates a full complete dataset that includes all the variables of interest that will be used\n",
    "## iloc function selects the relevant variables of interest based on column number\n",
    "## Problematic weather columns (i.e. don't select!): 6, 16, 17, 19, 20\n",
    "## Disease columns excluded due to limited dataset: 21:24, 25\n",
    "weatherclimateED['Date'] = pd.to_datetime(weatherclimateED['Date'])\n",
    "weatherclimateED['epiweek'] = weatherclimateED['Date'].apply(create_epiweek)\n",
    "weatherclimateED = weatherclimateED.set_index('epiweek')\n",
    "weatherclimateED = weatherclimateED.iloc[:, np.r_[30:32, 33:39, 40, 42 , 45:47, 49:51,  52:54, 1:6, 8:15]]\n",
    "weatherclimateED.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3f3296-097d-40b0-a7c2-6d598a519f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## This function takes the full dataset and creates an initial dataset with the specified range\n",
    "## also returns the name of the target variable for creation of the initial dataset\n",
    "## note disease_var here is an integer based off the column number\n",
    "def create_initial_dataset(dataset, disease_var: int):\n",
    "    explore_df = dataset.copy()\n",
    "    range_start = Week(2009,1)\n",
    "    range_end = Week (2018,52)\n",
    "    explore_df = explore_df.loc[range_start:range_end]\n",
    "    target_var = explore_df.columns.values.tolist()[disease_var]\n",
    "\n",
    "    if not os.path.exists(target_var):\n",
    "        os.makedirs(target_var)\n",
    "    path = os.path.join(target_var, F'initial_dataset.csv')\n",
    "    \n",
    "    explore_df.to_csv(path)\n",
    "    #explore_df1 is pure AR and explore_df2 is with environmetal vairables\n",
    "    explore_df_1 = explore_df[[target_var]] \n",
    "    explore_df_2 = pd.merge(explore_df[[target_var]], explore_df[explore_df.columns[16:28].to_list()], on='epiweek')\n",
    "#     explore_df_pure = explore_df.drop(columns=target_var)\n",
    "    return explore_df, explore_df_1, explore_df_2, target_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc107e1c-6d9b-4139-a100-b1726b6162b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_naive(dataset, step, target_var):\n",
    "    naive = dataset.copy()\n",
    "    naive = naive[[target_var]].shift(step)\n",
    "    return naive.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355bdb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_history_mean(dataset, lag, step, target_var):\n",
    "    origin_history_mean = dataset.copy()\n",
    "    history_mean = origin_history_mean[[target_var]].shift(step)\n",
    "    for i in range(step + 1, step + lag):\n",
    "        history_mean += origin_history_mean.shift(i)\n",
    "    return history_mean.dropna() / lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3b1deb-e97f-45d7-b5f7-6c4f296caea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lagged dataset\n",
    "def create_lagged_dataset(dataset, lag, target_var):\n",
    "    lagged_dataset = dataset.copy()\n",
    "    columns_list = list(lagged_dataset.columns)\n",
    "    data_join = {}\n",
    "    for column in columns_list:\n",
    "        if column == target_var:\n",
    "            data_join[column] = lagged_dataset[column]\n",
    "        for n in range(1,lag+1):\n",
    "            data_join[F'{column}_L{n}'] = lagged_dataset[column].shift(n)\n",
    "    lagged_dataset = pd.concat(data_join.values(), axis=1, ignore_index = True)\n",
    "    lagged_dataset.columns = data_join.keys()\n",
    "    return lagged_dataset.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3672d89c-abc7-4fa5-8e1b-5c6bfc8fffa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step is the number of weeks ahead that we are forecasting, e.g. step=2 is 2 weeks ahead.\n",
    "## Note step=1 results in no change to dataset, i.e. use generated lagged variables to forecast current. \n",
    "def create_stepped_dataset(dataset, step, target_var):\n",
    "    stepped_dataset = dataset.copy()\n",
    "    y = stepped_dataset[[target_var]].shift(-step+1)\n",
    "    if step != 1:\n",
    "        X = stepped_dataset.iloc[:-step+1, :]\n",
    "    else:\n",
    "        X = stepped_dataset\n",
    "    return X.drop(target_var, axis = 1), y.dropna()\n",
    "## So now target variable (y variable for exploration) is shifted back by 2 weeks. i.e., taking the y-value from 2 weeks later\n",
    "## and setting it to the current index. So linear regression of y+2 with the current X values. X will have\n",
    "## a smaller dataset with the last 2 time points removed because of the shift. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7112b902-bf3c-426a-ac21-d98a3bd3a673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_window(X, window_perc):\n",
    "    return X.index[0], X.index[int(len(X)*window_perc)]\n",
    "def create_output_dataset(y, window_end):\n",
    "    return y.copy().loc[window_end+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439f8bdf-0d86-42b3-a937-6094d334ff49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV, ElasticNet, ElasticNetCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from group_lasso import GroupLasso\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "# from nixtlats import TimeGPT\n",
    "from sklearn.decomposition import PCA\n",
    "from rpy2.robjects import pandas2ri, r\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import globalenv\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects import Formula\n",
    "from rpy2.robjects.conversion import localconverter\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "def block_bootstrap(data, block_size, num_resamples):\n",
    "    np.random.seed(0)\n",
    "    n = len(data)\n",
    "    num_blks = n - block_size + 1\n",
    "    resamples = []\n",
    "    blks = []\n",
    "    for i in range(num_blks):\n",
    "        start_idx = i\n",
    "        end_idx = start_idx + block_size\n",
    "        blks.append(data[start_idx:end_idx])\n",
    "    for _ in range(num_resamples):\n",
    "        resampled_series = []\n",
    "        resampled_indices = np.random.choice(num_blks, size=n // block_size + 1, replace=True)\n",
    "\n",
    "        for j in resampled_indices:\n",
    "            resampled_series.extend(blks[j])\n",
    "        resamples.append(np.array(resampled_series))\n",
    "    return resamples\n",
    "\n",
    "\n",
    "\n",
    "def coefs(model, coefs_path, filename):\n",
    "    coefs_path\n",
    "\n",
    "## This function runs the first order regression for the target disease, for one specified lag and step\n",
    "\n",
    "def regression_with_naive(X_dataset, y_dataset, X_dataset_1, y_dataset_1, X_dataset_2, y_dataset_2, window_start, window_end, y_pred_models, test_length, naive, history_mean, target_var, lag, step, block_size, num_bs, explore_dataset):\n",
    "    count = 0\n",
    "    df_end = X_dataset.index[-1]\n",
    "    while window_end < df_end:\n",
    "        if (window_end + 15) <= df_end:\n",
    "            expand_test_length = 15\n",
    "        else:\n",
    "            expand_test_length = X_dataset.index.get_loc(df_end) - X_dataset.index.get_loc(window_end) \n",
    "    \n",
    "        X = X_dataset.copy()\n",
    "        y = y_dataset.copy()\n",
    "        # Note: .loc is end-inclusive    \n",
    "        X_train = X.loc[window_start:window_end]\n",
    "        #print(X_train.info())\n",
    "        ## values.ravel() converts y_train to numpy array for compatibility with models (update: already deleted this)\n",
    "        y_train = y.loc[window_start:window_end]\n",
    "        #print(len(y_train))\n",
    "        ## double square brackets so X_test is extracted as a pandas df instead of series\n",
    "        X_test = X.loc[window_end+1:window_end+expand_test_length]\n",
    "        #print(X_test)\n",
    "        y_test = y.loc[window_end+1:window_end+expand_test_length]\n",
    "        #print(y_test)\n",
    "    \n",
    "        ## Scaling\n",
    "        scaler = StandardScaler()\n",
    "        ## .fit_transform stores the scaling parameters (fit), and transforms the training set\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        ## .transform takes the previously stored scaling parameters to transform the test set\n",
    "        ## Therefore, test set is transformed based on the training set parameters\n",
    "        X_test = scaler.transform(X_test)\n",
    "        # For all models using all variables, I use dataframe and not array\n",
    "        X_train = pd.DataFrame(X_train)\n",
    "        X_train.columns = X.columns\n",
    "        X_test = pd.DataFrame(X_test)\n",
    "        X_test.columns = X.columns\n",
    "\n",
    "        \n",
    "        ## data processing for pure AR \n",
    "        X_1 = X_dataset_1.copy()\n",
    "        y_1 = y_dataset_1.copy()\n",
    "        # Note: .loc is end-inclusive    \n",
    "        X_train_1 = X_1.loc[window_start:window_end]\n",
    "        y_train_1 = y_1.loc[window_start:window_end]\n",
    "        ## double square brackets so X_test is extracted as a pandas df instead of series\n",
    "        X_test_1 = X_1.loc[window_end+1:window_end+expand_test_length]\n",
    "        y_test_1 = y_1.loc[window_end+1:window_end+expand_test_length]\n",
    "    \n",
    "        ## Scaling\n",
    "        scaler = StandardScaler()\n",
    "        ## .fit_transform stores the scaling parameters (fit), and transforms the training set\n",
    "        X_train_1 = scaler.fit_transform(X_train_1)\n",
    "        ## .transform takes the previously stored scaling parameters to transform the test set\n",
    "        ## Therefore, test set is transformed based on the training set parameters\n",
    "        X_test_1 = scaler.transform(X_test_1)\n",
    "        # For pure factor model, I use dataframe and not array\n",
    "        X_train_1 = pd.DataFrame(X_train_1)\n",
    "        X_train_1.columns = X_1.columns\n",
    "        X_test_1 = pd.DataFrame(X_test_1)\n",
    "        X_test_1.columns = X_1.columns\n",
    "        \n",
    "        ## data processing for AR with environmental variables\n",
    "        X_2 = X_dataset_2.copy()\n",
    "        y_2 = y_dataset_2.copy()\n",
    "        # Note: .loc is end-inclusive    \n",
    "        X_train_2 = X_2.loc[window_start:window_end]\n",
    "        y_train_2 = y_2.loc[window_start:window_end]\n",
    "        ## double square brackets so X_test is extracted as a pandas df instead of series\n",
    "        X_test_2 = X_2.loc[window_end+1:window_end+expand_test_length]\n",
    "        y_test_2 = y_2.loc[window_end+1:window_end+expand_test_length]\n",
    "    \n",
    "        ## Scaling\n",
    "        scaler = StandardScaler()\n",
    "        ## .fit_transform stores the scaling parameters (fit), and transforms the training set\n",
    "        X_train_2 = scaler.fit_transform(X_train_2)\n",
    "        ## .transform takes the previously stored scaling parameters to transform the test set\n",
    "        ## Therefore, test set is transformed based on the training set parameters\n",
    "        X_test_2 = scaler.transform(X_test_2)\n",
    "        \n",
    "        X_train_2 = pd.DataFrame(X_train_2)\n",
    "        X_train_2.columns = X_2.columns\n",
    "        X_test_2 = pd.DataFrame(X_test_2)\n",
    "        X_test_2.columns = X_2.columns\n",
    "    \n",
    "        ## evaluate variance\n",
    "        \n",
    "        ## Implement cross-validation split\n",
    "        tscv = TimeSeriesSplit(n_splits = 5)\n",
    "        \n",
    "    \n",
    "        ## 1. Naive Forecast\n",
    "        residuals = [explore_dataset.loc[epiweek, target_var] - naive.loc[epiweek,target_var] for epiweek in naive.index]\n",
    "        resamples = block_bootstrap(residuals, block_size, num_bs)\n",
    "        resamples = np.array(resamples, dtype=object) # \"dtype=object\" to make sure the array can hold objects of any type\n",
    "        resamples  = resamples[:, :naive.shape[0]-resamples.shape[1]] # Excess samples are removed to align with the original data size\n",
    "        for i, resample in enumerate(resamples):\n",
    "            resample = pd.DataFrame(resample, index=naive.index, columns=naive.columns)\n",
    "            naive_tilde = resample + naive\n",
    "            naive_new = create_naive(naive_tilde, step, target_var)\n",
    "            y_pred_models['naive'].loc[window_end+1:window_end+expand_test_length, F'{i}'] = (naive_new.loc[window_end+1:window_end+expand_test_length, target_var]).values\n",
    "\n",
    "        \n",
    "        ## 2. Historical mean (rolling window = lag)\n",
    "        residuals = [explore_dataset.loc[epiweek, target_var] - history_mean.loc[epiweek,target_var] for epiweek in history_mean.index]\n",
    "        resamples = block_bootstrap(residuals, block_size, num_bs)\n",
    "        resamples = np.array(resamples, dtype=object) # \"dtype=object\" to make sure the array can hold objects of any type\n",
    "        resamples  = resamples[:, :history_mean.shape[0]-resamples.shape[1]] # Excess samples are removed to align with the original data size\n",
    "        for i, resample in enumerate(resamples):\n",
    "            resample = pd.DataFrame(resample, index=history_mean.index, columns=history_mean.columns)\n",
    "            history_mean_tilde = resample + history_mean\n",
    "            history_mean_new = create_history_mean(history_mean_tilde, lag, step, target_var)\n",
    "            y_pred_models['historymean'].loc[window_end+1:window_end+expand_test_length, F'{i}'] = (history_mean_new.loc[window_end+1:window_end+expand_test_length, target_var]).values\n",
    "\n",
    "        ## 3. Pure AR\n",
    "        ar_pure = LinearRegression()\n",
    "        ar_pure.fit(X_train_1, y_train_1)\n",
    "        \n",
    "        y_train_1_pred = ar_pure.predict(X_train_1)\n",
    "        residuals = [y_train_1.iloc[i,0] - y_train_1_pred[i] for i, epiweek in enumerate(X_train_1.index)]\n",
    "        resamples = block_bootstrap(residuals, block_size, num_bs)\n",
    "        resamples = np.array(resamples)\n",
    "        resamples  = resamples[:, :y_train_1_pred.shape[0]-resamples.shape[1]] # Excess samples are removed to align with the original data size\n",
    "        y_train_1_tilde = resamples + y_train_1_pred\n",
    "        # fit models on bootstrapping samples to estimate the empirical distribution\n",
    "        for i, sample in enumerate(y_train_1_tilde):\n",
    "            sample = pd.DataFrame(sample)\n",
    "            sample.index = y_train_1.index\n",
    "            sample.columns = y_train_1.columns\n",
    "            ar_pure_bs = LinearRegression()\n",
    "            ar_pure_bs.fit(X_train_1, sample)\n",
    "            y_pred_models['ar_pure'].loc[window_end+1:window_end+expand_test_length, F'{i}'] = ar_pure_bs.predict(X_test_1)\n",
    "        \n",
    "        \n",
    "        ## 4. AR only with environmental variables\n",
    "        ar_env = LinearRegression()\n",
    "        ar_env.fit(X_train_2, y_train_2)\n",
    "        \n",
    "        y_train_2_pred = ar_env.predict(X_train_2)\n",
    "        residuals = [y_train_2.iloc[i,0] - y_train_2_pred[i] for i, epiweek in enumerate(X_train_2.index)]\n",
    "        resamples = block_bootstrap(residuals, block_size, num_bs)\n",
    "        resamples = np.array(resamples)\n",
    "        resamples  = resamples[:, :y_train_2_pred.shape[0]-resamples.shape[1]] # Excess samples are removed to align with the original data size\n",
    "        y_train_2_tilde = resamples + y_train_2_pred\n",
    "        # fit models on bootstrapping samples to estimate the empirical distribution\n",
    "        for i, sample in enumerate(y_train_2_tilde):\n",
    "            sample = pd.DataFrame(sample)\n",
    "            sample.index = y_train_2.index\n",
    "            sample.columns = y_train_2.columns\n",
    "            ar_env_bs = LinearRegression()\n",
    "            ar_env_bs.fit(X_train_2, sample)\n",
    "            y_pred_models['ar_env'].loc[window_end+1:window_end+expand_test_length, F'{i}'] = ar_env_bs.predict(X_test_2)\n",
    "        \n",
    "\n",
    "    \n",
    "        ## 5. Ridge model\n",
    "        ridge_cv = RidgeCV(cv = tscv)\n",
    "        ridge_cv.fit(X_train, y_train)\n",
    "    \n",
    "        ridge_model = Ridge(alpha = ridge_cv.alpha_)\n",
    "        ridge_model.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = ridge_model.predict(X_train)\n",
    "        residuals = [y_train.iloc[i,0] - y_train_pred[i] for i, epiweek in enumerate(X_train.index)]\n",
    "        resamples = block_bootstrap(residuals, block_size, num_bs)\n",
    "        resamples = np.array(resamples)\n",
    "        resamples  = resamples[:, :y_train_pred.shape[0]-resamples.shape[1]] # Excess samples are removed to align with the original data size\n",
    "        y_train_tilde = resamples + y_train_pred\n",
    "        # fit models on bootstrapping samples to estimate the empirical distribution\n",
    "        for i, sample in enumerate(y_train_tilde):\n",
    "            sample = pd.DataFrame(sample)\n",
    "            sample.index = y_train.index\n",
    "            sample.columns = y_train.columns\n",
    "            \n",
    "            ridge_cv_bs = RidgeCV(cv = tscv)\n",
    "            ridge_cv_bs.fit(X_train, sample)\n",
    "            ridge_model_bs = Ridge(alpha = ridge_cv_bs.alpha_) \n",
    "            ridge_model_bs.fit(X_train, sample)\n",
    "            y_pred_models['ridge'].loc[window_end+1:window_end+expand_test_length, F'{i}'] = ridge_model_bs.predict(X_test)\n",
    "\n",
    "        \n",
    "        ## 6. Lasso Model\n",
    "        lasso_cv = LassoCV(cv = tscv, random_state = 18, max_iter = 100000)\n",
    "        lasso_cv.fit(X_train, y_train)\n",
    "        \n",
    "        # Create the Lasso model with the optimal alpha value\n",
    "        lasso_model = Lasso(alpha = lasso_cv.alpha_)\n",
    "        lasso_model.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = lasso_model.predict(X_train)\n",
    "        residuals = [y_train.iloc[i,0] - y_train_pred[i] for i, epiweek in enumerate(X_train.index)]\n",
    "        resamples = block_bootstrap(residuals, block_size, num_bs)\n",
    "        resamples = np.array(resamples)\n",
    "        resamples  = resamples[:, :y_train_pred.shape[0]-resamples.shape[1]] # Excess samples are removed to align with the original data size\n",
    "        y_train_tilde = resamples + y_train_pred\n",
    "        # fit models on bootstrapping samples to estimate the empirical distribution\n",
    "        for i, sample in enumerate(y_train_tilde):\n",
    "            sample = pd.DataFrame(sample)\n",
    "            sample.index = y_train.index\n",
    "            sample.columns = y_train.columns\n",
    "            \n",
    "            lasso_cv_bs = LassoCV(cv = tscv, random_state = 18)\n",
    "            lasso_cv_bs.fit(X_train, sample)\n",
    "            lasso_model_bs = Lasso(alpha = lasso_cv_bs.alpha_)\n",
    "            lasso_model_bs.fit(X_train, sample)\n",
    "            y_pred_models['lasso'].loc[window_end+1:window_end+expand_test_length, F'{i}'] = lasso_model_bs.predict(X_test)\n",
    "        \n",
    "        ## 7. Adaptive Lasso regression\n",
    "        linear_reg = LinearRegression()\n",
    "        linear_reg.fit(X_train, y_train)\n",
    "        initial_coef = linear_reg.coef_\n",
    "        # Calculate weights for the adaptive Lasso\n",
    "        weights = 1 / (np.abs(initial_coef) + 1e-5)\n",
    "        X_train_weighted = X_train / weights\n",
    "        \n",
    "        lasso_adaptive = Lasso(alpha = lasso_cv.alpha_)\n",
    "        lasso_adaptive.fit(X_train_weighted, y_train)\n",
    "        \n",
    "        lasso_adaptive.coef_ = lasso_adaptive.coef_ / weights\n",
    "        \n",
    "        y_train_pred = lasso_adaptive.predict(X_train)\n",
    "        residuals = [y_train.iloc[i,0] - y_train_pred[i] for i, epiweek in enumerate(X_train.index)]\n",
    "        resamples = block_bootstrap(residuals, block_size, num_bs)\n",
    "        resamples = np.array(resamples)\n",
    "        resamples  = resamples[:, :y_train_pred.shape[0]-resamples.shape[1]] # Excess samples are removed to align with the original data size\n",
    "        y_train_tilde = resamples + y_train_pred\n",
    "        # fit models on bootstrapping samples to estimate the empirical distribution\n",
    "        for i, sample in enumerate(y_train_tilde):\n",
    "            sample = pd.DataFrame(sample)\n",
    "            sample.index = y_train.index\n",
    "            sample.columns = y_train.columns\n",
    "            \n",
    "            linear_reg_bs = LinearRegression()\n",
    "            linear_reg_bs.fit(X_train, sample)\n",
    "            initial_coef_bs = linear_reg_bs.coef_\n",
    "            weights_bs = 1 / (np.abs(initial_coef_bs) + 1e-5)\n",
    "            X_train_weighted = X_train / weights_bs\n",
    "            lasso_cv_bs = LassoCV(cv = tscv, random_state = 18)\n",
    "            lasso_cv_bs.fit(X_train, sample)\n",
    "            lasso_adaptive_bs = Lasso(alpha = lasso_cv_bs.alpha_)\n",
    "            lasso_adaptive_bs.fit(X_train_weighted, sample)\n",
    "            lasso_adaptive_bs.coef_ = lasso_adaptive_bs.coef_ / weights_bs\n",
    "            y_pred_models['alasso'].loc[window_end+1:window_end+expand_test_length, F'{i}'] = lasso_adaptive_bs.predict(X_test)\n",
    "        \n",
    "        \n",
    "        \n",
    "        ## 8. Sparse Group Lasso regression\n",
    "        group_sizes = [8 for i in range(int(X_train.shape[1]/lag))]\n",
    "        groups = np.concatenate(\n",
    "            [size * [i] for i, size in enumerate(group_sizes)]\n",
    "        ).reshape(-1, 1)\n",
    "        \n",
    "        # Create the sgl model with the default group_reg and l1_reg\n",
    "        sgl_model = GroupLasso(groups=groups, random_state=18, scale_reg=\"inverse_group_size\", fit_intercept=True, n_iter=100000, supress_warning=True)\n",
    "        sgl_model.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = sgl_model.predict(X_train)\n",
    "        residuals = [y_train.iloc[i,0] - y_train_pred[i] for i, epiweek in enumerate(X_train.index)]\n",
    "        resamples = block_bootstrap(residuals, block_size, num_bs)\n",
    "        resamples = np.array(resamples)\n",
    "        resamples  = resamples[:, :y_train_pred.shape[0]-resamples.shape[1]] # Excess samples are removed to align with the original data size\n",
    "        y_train_tilde = resamples + y_train_pred\n",
    "        # fit models on bootstrapping samples to estimate the empirical distribution\n",
    "        for i, sample in enumerate(y_train_tilde):\n",
    "            sample = pd.DataFrame(sample)\n",
    "            sample.index = y_train.index\n",
    "            sample.columns = y_train.columns\n",
    "            sgl_model_bs = GroupLasso(groups=groups, random_state=18, scale_reg=\"inverse_group_size\", fit_intercept=True, n_iter=100000, supress_warning=True)\n",
    "            sgl_model_bs.fit(X_train, sample)\n",
    "            y_pred_models['sgl'].loc[window_end+1:window_end+expand_test_length, F'{i}'] = sgl_model_bs.predict(X_test)\n",
    "        \n",
    "        \n",
    "        ## 9. ElasticNet Model\n",
    "        elasticnet_cv = ElasticNetCV(cv = tscv, max_iter = 100000)\n",
    "        elasticnet_cv.fit(X_train, y_train)\n",
    "    \n",
    "        # Create the ElasticNet model with the optimal l1 and alpha values\n",
    "        elasticnet_model = ElasticNet(alpha = elasticnet_cv.alpha_, l1_ratio = elasticnet_cv.l1_ratio_)\n",
    "        elasticnet_model.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = elasticnet_model.predict(X_train)\n",
    "        residuals = [y_train.iloc[i,0] - y_train_pred[i] for i, epiweek in enumerate(X_train.index)]\n",
    "        resamples = block_bootstrap(residuals, block_size, num_bs)\n",
    "        resamples = np.array(resamples)\n",
    "        resamples  = resamples[:, :y_train_pred.shape[0]-resamples.shape[1]] # Excess samples are removed to align with the original data size\n",
    "        y_train_tilde = resamples + y_train_pred\n",
    "        # fit models on bootstrapping samples to estimate the empirical distribution\n",
    "        for i, sample in enumerate(y_train_tilde):\n",
    "            sample = pd.DataFrame(sample)\n",
    "            sample.index = y_train.index\n",
    "            sample.columns = y_train.columns\n",
    "            \n",
    "            elasticnet_cv_bs = ElasticNetCV(cv = tscv)\n",
    "            elasticnet_cv_bs.fit(X_train, sample)\n",
    "            elasticnet_model_bs = ElasticNet(alpha = elasticnet_cv_bs.alpha_, l1_ratio = elasticnet_cv_bs.l1_ratio_)\n",
    "            elasticnet_model_bs.fit(X_train, sample)\n",
    "            y_pred_models['elasticnet'].loc[window_end+1:window_end+expand_test_length, F'{i}'] = elasticnet_model_bs.predict(X_test)\n",
    "\n",
    "        \n",
    "        ## 10. Pure factor model\n",
    "#         print(X_train)\n",
    "        remove_names = []\n",
    "        for name in X_train.columns:\n",
    "            if name[0:-3] == target_var:\n",
    "                remove_names.append(name)\n",
    "\n",
    "        X_train_pure = X_train.drop(columns=remove_names)\n",
    "        X_test_pure = X_test.drop(columns=remove_names)\n",
    "        pca = PCA()\n",
    "        pca.fit(X_train_pure)\n",
    "        cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "          #to explain more than 85% of the variance\n",
    "        num_components = np.where(cumulative_variance_ratio >= 0.85)[0][0] + 1 \n",
    "        pca_new = PCA(n_components=num_components)\n",
    "        X_train_pca = pca_new.fit_transform(X_train_pure)\n",
    "        X_train_pca = pd.DataFrame(X_train_pca)\n",
    "        X_train_pca.columns = X_train_pca.columns.astype(str)\n",
    "        X_train_pca = pd.merge(X_train_1, X_train_pca, left_index=True, right_index=True)\n",
    "\n",
    "        X_test_pca = pca_new.transform(X_test_pure)\n",
    "        X_test_pca = pd.DataFrame(X_test_pca)\n",
    "        X_test_pca.columns = X_test_pca.columns.astype(str)\n",
    "        X_test_pca = pd.merge(X_test_1, X_test_pca, left_index=True, right_index=True)\n",
    "        \n",
    "        pure_factor_model = LinearRegression()\n",
    "        pure_factor_model.fit(X_train_pca, y_train)\n",
    "\n",
    "        y_train_pred = pure_factor_model.predict(X_train_pca)\n",
    "        residuals = [y_train.iloc[i,0] - y_train_pred[i] for i, epiweek in enumerate(X_train_pca.index)]\n",
    "        resamples = block_bootstrap(residuals, block_size, num_bs)\n",
    "        resamples = np.array(resamples)\n",
    "        resamples  = resamples[:, :y_train_pred.shape[0]-resamples.shape[1]] # Excess samples are removed to align with the original data size\n",
    "        y_train_tilde = resamples + y_train_pred\n",
    "        # fit models on bootstrapping samples to estimate the empirical distribution\n",
    "        for i, sample in enumerate(y_train_tilde):\n",
    "            sample = pd.DataFrame(sample)\n",
    "            sample.index = y_train.index\n",
    "            sample.columns = y_train.columns\n",
    "            pure_factor_model_bs = LinearRegression()\n",
    "            pure_factor_model_bs.fit(X_train_pca, sample)\n",
    "            y_pred_models['purefactor'].loc[window_end+1:window_end+expand_test_length, F'{i}'] = pure_factor_model_bs.predict(X_test_pca)\n",
    "\n",
    "    \n",
    "    \n",
    "        ## 11. KNN\n",
    "        knn_model = KNeighborsRegressor() #  default parameters\n",
    "        knn_model.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = knn_model.predict(X_train)\n",
    "        residuals = [y_train.iloc[i,0] - y_train_pred[i] for i, epiweek in enumerate(X_train.index)]\n",
    "        resamples = block_bootstrap(residuals, block_size, num_bs)\n",
    "        resamples = np.array(resamples)\n",
    "        resamples  = resamples[:, :y_train_pred.shape[0]-resamples.shape[1]] # Excess samples are removed to align with the original data size\n",
    "        y_train_tilde = resamples + y_train_pred\n",
    "        # fit models on bootstrapping samples to estimate the empirical distribution\n",
    "        for i, sample in enumerate(y_train_tilde):\n",
    "            sample = pd.DataFrame(sample)\n",
    "            sample.index = y_train.index\n",
    "            sample.columns = y_train.columns\n",
    "            knn_model_bs = KNeighborsRegressor()\n",
    "            knn_model_bs.fit(X_train, sample)\n",
    "            y_pred_models['knn'].loc[window_end+1:window_end+expand_test_length, F'{i}'] = knn_model_bs.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "        ## 12. XGBoost\n",
    "        xgboost_model = xgb.XGBRegressor(n_estimators=1000, random_state=18)\n",
    "        xgboost_model.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = xgboost_model.predict(X_train)\n",
    "        residuals = [y_train.iloc[i,0] - y_train_pred[i] for i, epiweek in enumerate(X_train.index)]\n",
    "        resamples = block_bootstrap(residuals, block_size, num_bs)\n",
    "        resamples = np.array(resamples)\n",
    "        resamples  = resamples[:, :y_train_pred.shape[0]-resamples.shape[1]] # Excess samples are removed to align with the original data size\n",
    "        y_train_tilde = resamples + y_train_pred\n",
    "        # fit models on bootstrapping samples to estimate the empirical distribution\n",
    "        for i, sample in enumerate(y_train_tilde):\n",
    "            sample = pd.DataFrame(sample)\n",
    "            sample.index = y_train.index\n",
    "            sample.columns = y_train.columns\n",
    "            xgboost_model_bs = xgb.XGBRegressor(n_estimators=1000, random_state=18)\n",
    "            xgboost_model_bs.fit(X_train, sample)\n",
    "            y_pred_models['xgboost'].loc[window_end+1:window_end+expand_test_length, F'{i}'] = xgboost_model_bs.predict(X_test)\n",
    "        \n",
    "    \n",
    "        ##\n",
    "        #keep track of model progress, every number of weeks\n",
    "        tracking_interval = 5\n",
    "        if window_end.weektuple()[1] % tracking_interval == 0:\n",
    "            print(F'{target_var} done with {window_end+expand_test_length}; {count} out of {test_length}')\n",
    "            \n",
    "        ## Implement expanding window\n",
    "        #window_start = window_start+1 (only for rolling window)\n",
    "        window_end += 15\n",
    "        count += 15\n",
    "\n",
    "    print(F'The last epiweek for {target_var} to be predicted is: {window_end}')\n",
    "    print(F'The total number of predicted epiweeks for {target_var} is: {count}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12edf56-31b5-4b42-a957-7df729f4892a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function sets up the first order regression for the target disease, for one specified lag and step\n",
    "\n",
    "def run_first_order_regression(dataset, dataset_1, dataset_2, lag, step, target_var, window_perc):\n",
    "    print(F'Running first order regression for {target_var} lag {lag} step {step}')\n",
    "    \n",
    "    naive = create_naive(dataset, step, target_var)\n",
    "    history_mean = create_history_mean(dataset, lag, step, target_var)\n",
    "    \n",
    "    lagged_dataset = create_lagged_dataset(dataset, lag, target_var)\n",
    "    lagged_dataset_1 = create_lagged_dataset(dataset_1, lag, target_var)\n",
    "    lagged_dataset_2 = create_lagged_dataset(dataset_2, lag, target_var)\n",
    "    \n",
    "    X, y = create_stepped_dataset(lagged_dataset, step, target_var)\n",
    "    X_1, y_1 = create_stepped_dataset(lagged_dataset_1, step, target_var)\n",
    "    X_2, y_2 = create_stepped_dataset(lagged_dataset_2, step, target_var)\n",
    "    \n",
    "    window_start, window_end = create_window(X, window_perc)\n",
    "\n",
    "    print(F'The first epiweek to be predicted for {target_var} lag {lag} step {step} is: {window_end+1}')\n",
    "    \n",
    "    num_bs = 1000 # Bootstrapping times\n",
    "    block_size = 20\n",
    "    model_list = ['naive', 'historymean', 'ar_pure', 'ar_env', 'ridge', 'lasso', 'alasso', 'sgl', \n",
    "                 'elasticnet', 'purefactor', 'knn', 'xgboost']\n",
    "    \n",
    "    y_pred_models = {key: create_output_dataset(y, window_end) for key in model_list}\n",
    "    \n",
    "\n",
    "    train_length = len(X.loc[window_start:window_end])\n",
    "    print(F'The initial training dataset length for {target_var} lag {lag} step {step} is: {train_length}')\n",
    "\n",
    "\n",
    "    test_length = len(X.loc[window_end+1:])\n",
    "    print(F'The initial testing dataset length for {target_var} lag {lag} step {step} is: {test_length}')\n",
    "\n",
    "        \n",
    "    regression_with_naive(X, y, X_1, y_1, X_2, y_2, window_start, window_end, y_pred_models, test_length, naive, history_mean, target_var, lag, step, block_size, num_bs, dataset)\n",
    "    \n",
    "    pred_path = os.path.join(target_var, 'pred', F'L{lag}_S{step}')\n",
    "    if not os.path.exists(pred_path):\n",
    "        os.makedirs(pred_path)\n",
    "    \n",
    "    for model in model_list:\n",
    "        pred_model_path = os.path.join(pred_path, F'{model}.csv')\n",
    "        y_pred_models[model].to_csv(pred_model_path)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    print(F'Completed for {target_var} lag {lag} step {step}')\n",
    "    clear_output(wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1d5bb5-c6e2-44dd-bab9-fb440ba32173",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function runs the regression for one disease, for all lags and steps, hence the for loop\n",
    "\n",
    "def run_disease_regression(dataset, disease_var, lag, step):\n",
    "    \n",
    "    ## Note how the integer disease_var is input into this function, and then\n",
    "    ## the string target_var is returned for the remaining functions\n",
    "    explore_df, explore_df_1, explore_df_2, target_var = create_initial_dataset(dataset, disease_var)\n",
    "\n",
    "    with open(\"target_variables.txt\") as target_variables_file:\n",
    "        if target_var not in target_variables_file.read():\n",
    "            with open(\"target_variables.txt\", 'a') as target_variables_file:\n",
    "                target_variables_file.write(F'{target_var}\\n')\n",
    "    \n",
    "    ## run the first order regression for all lags and steps for this target variable\n",
    "    print(F'Running regression for {target_var}')\n",
    "    run_first_order_regression(explore_df, explore_df_1, explore_df_2, lag = lag, step = step, target_var = target_var, window_perc = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6787b52d-4052-4a42-9be8-b5e233ea441e",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "## Main function call using Parallel\n",
    "## x in range (0,16) represents the 16 diseases that are the target variables. However, for this function we input them as integers\n",
    "## the create_initial_dataset function will convert the integer format to string format\n",
    "## Using parallel, each disease can be run on one computer core\n",
    "combinations = list(itertools.product(range(0, 16), range(1, 13)))\n",
    "Parallel(n_jobs=-1, verbose=51)(delayed(run_disease_regression)(weatherclimateED, x, 8, y) for x, y in combinations)\n",
    "#run_full_regression(weatherclimateED, range(0,16), 8, 9, 1, 9)\n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da6b2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('finish')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
