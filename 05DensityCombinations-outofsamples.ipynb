{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b9f97c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, mean_absolute_error\n",
    "import itertools\n",
    "import statsmodels.api as sm\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "import properscoring as ps\n",
    "from scipy.stats import norm\n",
    "import properscoring as ps\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import norm\n",
    "from scipy.integrate import simps\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.interpolate import interp1d\n",
    "import warnings\n",
    "\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\" # this change is only for this time's session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d576bf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from epiweeks import Week, Year\n",
    "from datetime import date\n",
    "def create_epiweek(date):\n",
    "    return Week.fromdate(date)\n",
    "def create_epiweekplot(epiweek):\n",
    "    epiweek = str(epiweek)\n",
    "    return F'Y{epiweek[:4]}W{epiweek[4:]}'\n",
    "def filename_to_epiweek(filename):\n",
    "    return Week.fromstring(F'{filename[:4]}W{filename[4:6]}')\n",
    "def create_epiweek_fromstr(str):\n",
    "    return Week.fromstring(str)\n",
    "def create_epiweek_fromint(int):\n",
    "    return Week.fromstring(str(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05ab27e",
   "metadata": {},
   "source": [
    "### Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "59ea15de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def empirical_cdf(data):\n",
    "    sorted_data = np.sort(data)\n",
    "    cdf = np.arange(1, len(data) + 1) / len(data)\n",
    "    return sorted_data, cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8e6b21ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def linear_pool(weights, y_pred_epiweek_all_models, models):\n",
    "#     np.random.seed(0)\n",
    "#     weights /= np.sum(weights)\n",
    "    \n",
    "#     for key in y_pred_epiweek_all_models.keys():\n",
    "#         y_pred_epiweek_all_models[key] = y_pred_epiweek_all_models[key].to_frame().T\n",
    "\n",
    "#     samples_prob = np.random.choice(models, size=1000, p=weights) # n_samples = 1000, n_submodels = 4\n",
    "\n",
    "#     combined_samples = np.zeros(1000)\n",
    "#     for i in range(1000):\n",
    "#         density_submodel = y_pred_epiweek_all_models[samples_prob[i]].iloc[0,1:].values\n",
    "#         combined_samples[i] = np.random.choice(density_submodel)\n",
    "\n",
    "    \n",
    "#     percentiles = [2.5, 97.5]   # for 95 CI and visualization \n",
    "#     bound = np.percentile(combined_samples, percentiles)\n",
    "    \n",
    "#     y_val_value = y_pred_epiweek_all_models[models[0]].iloc[0,0]\n",
    "#     y_val_index = y_pred_epiweek_all_models[models[0]].index   # becasue the epiweek is same for all submodels. so we just choose models[0]\n",
    "    \n",
    "#     return combined_samples, y_val_value, y_val_index, bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "42c36fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_pool(weights, y_pred_epiweek_all_models, models):\n",
    "    np.random.seed(0)\n",
    "    weights /= np.sum(weights)\n",
    "    \n",
    "    cdf_list = []\n",
    "    x_min = float('inf')\n",
    "    x_max = float('-inf')\n",
    "    for key in models:\n",
    "        y_pred_epiweek_all_models[key] = y_pred_epiweek_all_models[key].to_frame().T\n",
    "        x, cdf = empirical_cdf(y_pred_epiweek_all_models[key].iloc[0,1:].values)\n",
    "        cdf_list.append((x, cdf))\n",
    "        x_min = min(x_min, x.min())\n",
    "        x_max = max(x_max, x.max())\n",
    "        \n",
    "    x_values = np.linspace(x_min, x_max, 1000)\n",
    "    \n",
    "    # Interpolate each CDF over the common x_values\n",
    "    cdf_interp_list = []\n",
    "    for x, cdf in cdf_list:\n",
    "        f = interp1d(x, cdf, kind='linear', bounds_error=False, fill_value=(0, 1))\n",
    "        cdf_interp = f(x_values)\n",
    "        cdf_interp_list.append(cdf_interp)    \n",
    "    \n",
    "    combined_cdf = np.zeros_like(x_values)\n",
    "    for weight, cdf_interp in zip(weights, cdf_interp_list):\n",
    "        combined_cdf += weight * cdf_interp\n",
    "    \n",
    "    \n",
    "    uniform_samples = np.random.uniform(0, 1, 1000) # uniform samples ensures that the generated empirical points follow the combined distribution accurately.\n",
    "\n",
    "    inverse_cdf = interp1d(combined_cdf, x_values, bounds_error=False, fill_value=(x_values.min(), x_values.max()))\n",
    "    combined_samples = inverse_cdf(uniform_samples)\n",
    "       \n",
    "    \n",
    "    y_val_value = y_pred_epiweek_all_models[models[0]].iloc[0,0]\n",
    "    y_val_index = y_pred_epiweek_all_models[models[0]].index\n",
    "    \n",
    "    return combined_samples, y_val_value, y_val_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "661f4ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def harmonic_pool(weights, y_pred_epiweek_all_models, models):\n",
    "    np.random.seed(0)\n",
    "    weights /= np.sum(weights)\n",
    "    \n",
    "    cdf_list = []\n",
    "    x_min = float('inf')\n",
    "    x_max = float('-inf')\n",
    "    for key in models:\n",
    "        y_pred_epiweek_all_models[key] = y_pred_epiweek_all_models[key].to_frame().T\n",
    "        x, cdf = empirical_cdf(y_pred_epiweek_all_models[key].iloc[0,1:].values)\n",
    "        cdf_list.append((x, cdf))\n",
    "        x_min = min(x_min, x.min())\n",
    "        x_max = max(x_max, x.max())\n",
    "        epsilon = 1e-10\n",
    "        \n",
    "    x_values = np.linspace(x_min, x_max, 1000)\n",
    "    \n",
    "    # Interpolate each CDF over the common x_values\n",
    "    cdf_interp_list = []\n",
    "    for x, cdf in cdf_list:\n",
    "        f = interp1d(x, cdf, kind='linear', bounds_error=False, fill_value=(0, 1))\n",
    "        cdf_interp = f(x_values) + epsilon\n",
    "        cdf_interp_list.append(cdf_interp)    \n",
    "    \n",
    "    combined_cdf_inv = np.zeros_like(x_values)\n",
    "    for weight, cdf_interp in zip(weights, cdf_interp_list):\n",
    "        combined_cdf_inv += weight * 1/cdf_interp\n",
    "    combined_cdf = 1 / combined_cdf_inv\n",
    "    \n",
    "    uniform_samples = np.random.uniform(0, 1, 1000) # uniform samples ensures that the generated empirical points follow the combined distribution accurately.? (check here)\n",
    "\n",
    "    inverse_cdf = interp1d(combined_cdf, x_values, bounds_error=False, fill_value=(x_values.min(), x_values.max()))\n",
    "    combined_samples = inverse_cdf(uniform_samples)\n",
    "       \n",
    "    \n",
    "    y_val_value = y_pred_epiweek_all_models[models[0]].iloc[0,0]\n",
    "    y_val_index = y_pred_epiweek_all_models[models[0]].index\n",
    "    \n",
    "    return combined_samples, y_val_value, y_val_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "57d179e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logarithmic_pool(weights, y_pred_epiweek_all_models, models):\n",
    "    np.random.seed(0)\n",
    "    weights /= np.sum(weights)\n",
    "    \n",
    "    cdf_list = []\n",
    "    x_min = float('inf')\n",
    "    x_max = float('-inf')\n",
    "    for key in models:\n",
    "        y_pred_epiweek_all_models[key] = y_pred_epiweek_all_models[key].to_frame().T\n",
    "        x, cdf = empirical_cdf(y_pred_epiweek_all_models[key].iloc[0,1:].values)\n",
    "        cdf_list.append((x, cdf))\n",
    "        x_min = min(x_min, x.min())\n",
    "        x_max = max(x_max, x.max())\n",
    "        \n",
    "    x_values = np.linspace(x_min, x_max, 1000)\n",
    "    \n",
    "    # Interpolate each CDF over the common x_values\n",
    "    cdf_interp_list = []\n",
    "    for x, cdf in cdf_list:\n",
    "        f = interp1d(x, cdf, kind='linear', bounds_error=False, fill_value=(0, 1))\n",
    "        cdf_interp = f(x_values)\n",
    "        cdf_interp_list.append(cdf_interp)    \n",
    "    \n",
    "    combined_cdf = np.ones_like(x_values)\n",
    "    for weight, cdf_interp in zip(weights, cdf_interp_list):\n",
    "        combined_cdf *= np.power(cdf_interp, weight)\n",
    "    \n",
    "    \n",
    "    uniform_samples = np.random.uniform(0, 1, 1000) # uniform samples ensures that the generated empirical points follow the combined distribution accurately.\n",
    "\n",
    "    inverse_cdf = interp1d(combined_cdf, x_values, bounds_error=False, fill_value=(x_values.min(), x_values.max()))\n",
    "    combined_samples = inverse_cdf(uniform_samples)\n",
    "        \n",
    "    \n",
    "    y_val_value = y_pred_epiweek_all_models[models[0]].iloc[0,0]\n",
    "    y_val_index = y_pred_epiweek_all_models[models[0]].index\n",
    "    \n",
    "    return combined_samples, y_val_value, y_val_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d91ce9",
   "metadata": {},
   "source": [
    "### Quantile combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d3a078cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def empirical_quantile_function(data):\n",
    "    sorted_data = np.sort(data)\n",
    "    cdf = np.arange(1, len(data) + 1) / len(data)\n",
    "    quantile_function = interp1d(cdf, sorted_data, kind='linear', bounds_error=False, fill_value=(sorted_data[0], sorted_data[-1]))\n",
    "    \n",
    "    return quantile_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "12dcda39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_comb(weights, y_pred_epiweek_all_models, models):\n",
    "    np.random.seed(0)\n",
    "    weights /= np.sum(weights)\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    quantile_function_list = []\n",
    "    for key in models:\n",
    "        y_pred_epiweek_all_models[key] = y_pred_epiweek_all_models[key].to_frame().T\n",
    "        quantile_function = empirical_quantile_function(y_pred_epiweek_all_models[key].iloc[0,1:].values)\n",
    "        quantile_function_list.append(quantile_function)\n",
    "        \n",
    "    y_samples = np.random.uniform(0, 1, 1000) # we approximate the integral by simulations from a uniform distribution (1000 times)\n",
    "    \n",
    "    # Interpolate each quantile functiontion over the common y_samples\n",
    "    quantiles_list = []\n",
    "    for quantile_function in quantile_function_list:\n",
    "        quantiles = quantile_function(y_samples)\n",
    "        quantiles_list.append(quantiles) \n",
    "        \n",
    "    combined_quantiles = np.zeros_like(y_samples)\n",
    "    for weight, quantiles in zip(weights, quantiles_list):\n",
    "        combined_quantiles += weight * quantiles\n",
    "    \n",
    "\n",
    "    combined_samples = combined_quantiles\n",
    "        \n",
    "    \n",
    "    y_val_value = y_pred_epiweek_all_models[models[0]].iloc[0,0]\n",
    "    y_val_index = y_pred_epiweek_all_models[models[0]].index\n",
    "    \n",
    "    return combined_samples, y_val_value, y_val_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833cb63a",
   "metadata": {},
   "source": [
    "### scoring rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c946fb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRPS for linear pooling\n",
    "def crps(combined_samples, y_val_value, y_val_index):\n",
    "    \n",
    "    crps_df = pd.DataFrame(columns=['CRPS'])\n",
    "    \n",
    "    for i, epiweek in enumerate(y_val_index): \n",
    "        \n",
    "        crps_df.at[epiweek, 'CRPS'] = ps.crps_ensemble(y_val_value, combined_samples)\n",
    "\n",
    "    return crps_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "91775362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def dss(combined_samples, y_val_value, y_val_index):\n",
    "\n",
    "#     # Estimate mean and variance\n",
    "#     mean_estimated = np.mean(combined_samples)\n",
    "#     variance_estimated = np.var(combined_samples, ddof=1) # ddof=1: to get an unbiased estimate\n",
    "    \n",
    "#     dss_df = pd.DataFrame(columns=['DSS'])\n",
    "\n",
    "#     for i, epiweek in enumerate(y_val_index):\n",
    "#         # Ensure that the variance is positive and non-zero\n",
    "#         variance = np.maximum(variance_estimated, 1e-6)\n",
    "\n",
    "#         # Calculate DSS for the current epiweek and model\n",
    "#         dss = ((y_val_value - mean_estimated)**2 / variance) + np.log(variance)\n",
    "#         dss_df.at[epiweek,'DSS'] = dss\n",
    "\n",
    "#     return dss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1801dd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(combined_samples, y_val_value, y_val_index):\n",
    "\n",
    "    log_df = pd.DataFrame(columns=['LogScore'])\n",
    "\n",
    "    for i, epiweek in enumerate(y_val_index):\n",
    "        # Estimate density of combined samples\n",
    "        kde = gaussian_kde(combined_samples)\n",
    "        prob_density = kde(y_val_value)\n",
    "        prob_density = max(prob_density, 1e-9)  # To avoid log(0)\n",
    "\n",
    "        log_score = -np.log(prob_density)\n",
    "        log_df.at[epiweek, 'LogScore'] = log_score\n",
    "\n",
    "    return log_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfed38d6",
   "metadata": {},
   "source": [
    "### combinatnion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f9f5ce9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combinations(target_var, pred_directory, density_forecast_directory, density_weights_directory, crps_directory, combi_samples_directory, suffix):\n",
    "    models = ['naive', 'historymean', 'ar_pure', 'ar_env', 'ridge', 'lasso', 'alasso', 'sgl',\n",
    "                 'elasticnet', 'purefactor', 'knn', 'xgboost']\n",
    "    pred_directory_path = os.path.join(target_var, pred_directory)\n",
    "    density_forecast_directory_path = os.path.join(target_var, density_forecast_directory)\n",
    "    density_weights_path = os.path.join(target_var, density_weights_directory)\n",
    "    crps_directory_path = os.path.join(target_var, crps_directory)\n",
    "    combi_samples_directory_path = os.path.join(target_var, combi_samples_directory)\n",
    "    \n",
    "    if not os.path.exists(density_forecast_directory_path):\n",
    "        os.makedirs(density_forecast_directory_path)\n",
    "    if not os.path.exists(density_weights_path):\n",
    "        os.makedirs(density_weights_path)\n",
    "    if not os.path.exists(combi_samples_directory_path):\n",
    "        os.makedirs(combi_samples_directory_path)\n",
    "\n",
    "    pooling_methods = {\n",
    "        f'linearpool_{suffix}': linear_pool,\n",
    "        f'harmonicpool_{suffix}': harmonic_pool,\n",
    "        f'logpool_{suffix}': logarithmic_pool,\n",
    "        f'vincentization_{suffix}': quantile_comb\n",
    "    }        \n",
    "\n",
    "    for step_name in os.listdir(pred_directory_path):\n",
    "        output_dir = os.path.join(combi_samples_directory_path, step_name)\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            \n",
    "        pred_models_path = os.path.join(pred_directory_path, step_name)\n",
    "        if os.path.isdir(pred_models_path):\n",
    "            crps_file = os.path.join(crps_directory_path, F'{step_name}.csv')\n",
    "            full_crps = pd.read_csv(crps_file, parse_dates = [0], dayfirst = True, index_col = 0)\n",
    "            full_crps['epiweek'] = full_crps.index\n",
    "            full_crps['epiweek'] = full_crps['epiweek'].apply(create_epiweek_fromint)\n",
    "            full_crps = full_crps.set_index('epiweek')\n",
    "            full_crps = full_crps[models]\n",
    "\n",
    "            all_weights = []\n",
    "            for i, epiweek in enumerate(full_crps.index):\n",
    "                inverse_crps = 1 / full_crps.loc[epiweek]\n",
    "                sum_of_inverses = inverse_crps.sum(axis=0)\n",
    "                weights = inverse_crps / sum_of_inverses\n",
    "                weights = np.array(weights)\n",
    "                all_weights.append(weights)\n",
    "\n",
    "            crps_density_forecast_df = pd.DataFrame(index=full_crps.index)\n",
    "            log_density_forecast_df = pd.DataFrame(index=full_crps.index)\n",
    "\n",
    "            \n",
    "            combined_samples = {\n",
    "                f'linearpool_{suffix}': [],\n",
    "                f'harmonicpool_{suffix}': [],\n",
    "                f'logpool_{suffix}': [],\n",
    "                f'vincentization_{suffix}': [],\n",
    "            }\n",
    "            \n",
    "            for pooling_method_name, pooling_method_func in pooling_methods.items():\n",
    "                crps_col = pd.DataFrame(columns=['CRPS'])\n",
    "                log_col = pd.DataFrame(columns=['log'])    \n",
    "\n",
    "\n",
    "                for i, epiweek in enumerate(full_crps.index):\n",
    "                    y_pred_epiweek_all_models = {} # dictionary to store the all submodels's pred at one epiweek\n",
    "                    for model_name in os.listdir(pred_models_path): # 'model_name' here includes the '.csv'\n",
    "                        pred_file = os.path.join(pred_models_path, model_name) # the order of names of models may be shuffled here.\n",
    "                        model = model_name[0:-4]\n",
    "\n",
    "                        if os.path.isfile(pred_file):\n",
    "                            y_pred = pd.read_csv(pred_file, parse_dates = [0], dayfirst = True)  \n",
    "                            y_pred['epiweek'] = y_pred['epiweek'].apply(create_epiweek_fromstr)\n",
    "                            y_pred = y_pred.set_index('epiweek')\n",
    "                            y_pred_epiweek = y_pred.loc[epiweek]\n",
    "                            y_pred_epiweek_all_models[model] = y_pred_epiweek\n",
    "                            \n",
    "                    combi_result = pooling_method_func(all_weights[i], y_pred_epiweek_all_models.copy(), models)\n",
    "                    combi_result_df = pd.DataFrame(combi_result[0].reshape(1, -1))\n",
    "                    combi_result_df.insert(0, f'{target_var}', y_pred_epiweek_all_models['ar_pure'].to_frame().T.iloc[0,0])\n",
    "                    combi_result_df.insert(0, 'epiweek', str(epiweek))\n",
    "                    combined_samples[pooling_method_name].append(combi_result_df)                    \n",
    "\n",
    "                    crps_result = crps(combi_result[0], combi_result[1], combi_result[2])\n",
    "                    log_result = log(combi_result[0], combi_result[1], combi_result[2])\n",
    "                    crps_col.at[epiweek, 'CRPS'] = crps_result.iloc[0,0] # because there is only 1 element in this df\n",
    "                    log_col.at[epiweek, 'log'] = log_result.iloc[0,0]\n",
    "                \n",
    "                # save the empirical distributinons to .csv\n",
    "                combined_samples_df = pd.concat(combined_samples[pooling_method_name], ignore_index=True)    \n",
    "                output_file = os.path.join(output_dir, f'{pooling_method_name}.csv')\n",
    "                combined_samples_df.to_csv(output_file, index=False)\n",
    "\n",
    "                crps_density_forecast_df = pd.concat([crps_density_forecast_df, crps_col], axis=1)\n",
    "                log_density_forecast_df = pd.concat([log_density_forecast_df, log_col], axis=1)\n",
    "            crps_density_forecast_df.columns = pooling_methods.keys()\n",
    "            log_density_forecast_df.columns = pooling_methods.keys()\n",
    "\n",
    "\n",
    "\n",
    "            density_forecast_output = pd.DataFrame()\n",
    "            for col in crps_density_forecast_df.columns:\n",
    "                density_forecast_output.at[col, 'crps_DENSITY_FORECAST'] = crps_density_forecast_df[col].mean()\n",
    "                density_forecast_output.at[col, 'log_DENSITY_FORECAST'] = log_density_forecast_df[col].mean()\n",
    "            density_forecast_output.to_csv(os.path.join(density_forecast_directory_path, F'{step_name}.csv'), mode='a', header=False)\n",
    "\n",
    "\n",
    "            all_weights = pd.DataFrame(all_weights)\n",
    "            all_weights.index = full_crps.index\n",
    "            all_weights.columns = models\n",
    "            all_weights.to_csv(os.path.join(density_weights_path, F'{step_name}.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f2e2ee2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cardiovascular disease', 'Chronic respiratory disease', 'Factors influencing health status and contact with health services', 'Digestive disease', 'Endocrine disorders', 'Malignant neoplasms', 'Diabetes mellitus', 'Genitourinary disorders', 'Musculoskeletal disease', 'Infectious and Parasitic Diseases', 'Neurological and sense disorders', 'Oral Diseases', 'Other neoplasms', 'Respiratory Infection', 'Skin diseases']\n",
      "Running with crps_directory: full_crps_P3 and density_weights_directory: density_weights_P3\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed: 29.3min\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of  15 | elapsed: 29.5min remaining: 191.9min\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  15 | elapsed: 29.7min remaining: 118.6min\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  15 | elapsed: 29.7min remaining: 81.7min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  15 | elapsed: 29.8min remaining: 59.7min\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  15 | elapsed: 29.9min remaining: 44.9min\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  15 | elapsed: 30.3min remaining: 34.6min\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  15 | elapsed: 31.5min remaining: 27.6min\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of  15 | elapsed: 37.7min remaining: 25.1min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  15 | elapsed: 37.8min remaining: 18.9min\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  15 | elapsed: 37.8min remaining: 13.7min\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  15 | elapsed: 38.9min remaining:  9.7min\n",
      "[Parallel(n_jobs=-1)]: Done  13 out of  15 | elapsed: 50.9min remaining:  7.8min\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed: 51.6min finished\n",
      "Running with crps_directory: full_crps_P2 and density_weights_directory: density_weights_P2\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed: 30.6min\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of  15 | elapsed: 31.9min remaining: 207.2min\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  15 | elapsed: 32.0min remaining: 127.9min\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  15 | elapsed: 32.1min remaining: 88.2min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  15 | elapsed: 32.1min remaining: 64.2min\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  15 | elapsed: 32.2min remaining: 48.3min\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  15 | elapsed: 32.3min remaining: 36.9min\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  15 | elapsed: 32.4min remaining: 28.4min\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of  15 | elapsed: 32.4min remaining: 21.6min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  15 | elapsed: 32.5min remaining: 16.3min\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  15 | elapsed: 33.2min remaining: 12.1min\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  15 | elapsed: 33.3min remaining:  8.3min\n",
      "[Parallel(n_jobs=-1)]: Done  13 out of  15 | elapsed: 54.0min remaining:  8.3min\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed: 54.1min finished\n",
      "Running with crps_directory: full_crps_P1 and density_weights_directory: density_weights_P1\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed: 28.6min\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of  15 | elapsed: 28.9min remaining: 188.0min\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  15 | elapsed: 29.4min remaining: 117.5min\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  15 | elapsed: 35.1min remaining: 96.6min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  15 | elapsed: 35.8min remaining: 71.5min\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  15 | elapsed: 36.1min remaining: 54.1min\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  15 | elapsed: 36.3min remaining: 41.5min\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  15 | elapsed: 36.5min remaining: 32.0min\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of  15 | elapsed: 36.7min remaining: 24.5min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  15 | elapsed: 36.7min remaining: 18.4min\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  15 | elapsed: 37.9min remaining: 13.8min\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  15 | elapsed: 38.0min remaining:  9.5min\n",
      "[Parallel(n_jobs=-1)]: Done  13 out of  15 | elapsed: 52.6min remaining:  8.1min\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed: 53.1min finished\n"
     ]
    }
   ],
   "source": [
    "def run_full_combinations_multiple(target_variables_file, pred_directory, density_forecast_directory, density_weights_directory_base, crps_directory_base, combi_samples_directory):\n",
    "    target_variables = []\n",
    "    with open(target_variables_file, 'r') as file:\n",
    "        for line in file:\n",
    "            target_variable = line.strip()\n",
    "            target_variables.append(target_variable)\n",
    "    print(target_variables)\n",
    "\n",
    "    # List of suffixes for the directories\n",
    "    suffixes = ['P3', 'P2', 'P1']\n",
    "\n",
    "    for suffix in suffixes:\n",
    "        density_weights_directory = f\"{density_weights_directory_base}_{suffix}\"\n",
    "        crps_directory = f\"{crps_directory_base}_{suffix}\"\n",
    "        \n",
    "        print(f\"Running with crps_directory: {crps_directory} and density_weights_directory: {density_weights_directory}\")\n",
    "        \n",
    "\n",
    "        Parallel(n_jobs=-1, verbose=51)(\n",
    "            delayed(combinations)(\n",
    "                target_var, \n",
    "                pred_directory, \n",
    "                density_forecast_directory, \n",
    "                density_weights_directory, \n",
    "                crps_directory, \n",
    "                combi_samples_directory,\n",
    "                suffix\n",
    "            ) for target_var in target_variables\n",
    "        )\n",
    "\n",
    "run_full_combinations_multiple(\n",
    "    'target_variables_new.txt', \n",
    "    'pred', \n",
    "    'density_forecast_metrics', \n",
    "    'density_weights',    \n",
    "    'full_crps',          \n",
    "    'combi_samples'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9817c20",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40659a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb = pd.read_csv('Cardiovascular disease/pred/L8_S6/naive.csv', parse_dates = [0], dayfirst = True)\n",
    "sb['epiweek'] = sb['epiweek'].apply(create_epiweek_fromstr)\n",
    "sb = sb.set_index('epiweek') \n",
    "sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e06a8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "haha = sb.loc[sb.index[1]]\n",
    "haha = haha.to_frame().T\n",
    "haha.iloc[0,1:].values\n",
    "haha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc2ece6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice(haha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3068d091",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_crps = 1 / hh.loc[hh.index[0]]\n",
    "sum_of_inverses = inverse_crps.sum(axis=0)\n",
    "weights = inverse_crps / sum_of_inverses\n",
    "weights = np.array(weights)\n",
    "a = []\n",
    "a.append(weights)\n",
    "a.append(weights)\n",
    "a = pd.DataFrame(a)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b451fb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb2 = pd.read_csv('Cardiovascular disease/full_crps/L8_S6.csv', parse_dates = [0], dayfirst = True, index_col = 0)\n",
    "sb2['epiweek'] = sb2.index\n",
    "sb2['epiweek'] = sb2['epiweek'].apply(create_epiweek_fromint)\n",
    "sb2= sb2.set_index('epiweek')\n",
    "sb2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50466e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-10, 10, 10000)\n",
    "weights = np.array([0.3, 0.5, 0.2])\n",
    "pdf_all = []\n",
    "pdf_1 = lambda x: norm.pdf(x, 0, 1)\n",
    "pdf_2 = lambda x: norm.pdf(x, 1, 2)\n",
    "pdf_3 = lambda x: norm.pdf(x, 2, 1)\n",
    "pdf_all.append(pdf_1)\n",
    "pdf_all.append(pdf_2)\n",
    "pdf_all.append(pdf_3)\n",
    "combined_pdf = np.sum([p(x) * w for p,w in zip(pdf_all, weights)], axis=0)\n",
    "# Create the plot\n",
    "plt.plot(x, combined_pdf)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Probability Density')\n",
    "\n",
    "mean_estimated = simps(x * combined_pdf, x) / simps(combined_pdf, x)\n",
    "print(mean_estimated)\n",
    "print(simps((x - mean_estimated)**2 * combined_pdf, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913ef521",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = 0.3 * np.random.normal(0, 1, 10000) + \\\n",
    "    0.5 * np.random.normal(1, 2, 10000) + \\\n",
    "    0.2 * np.random.normal(2, 1, 10000)\n",
    "\n",
    "# Calculate the sample mean\n",
    "mean_Z = np.mean(Z)\n",
    "\n",
    "# Calculate the sample variance\n",
    "variance_Z = np.var(Z, ddof=1)\n",
    "\n",
    "print(f\"Estimated Mean of Z: {mean_Z}\")\n",
    "print(f\"Estimated Variance of Z: {variance_Z}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03eef162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.linspace(-100, 100, 100000)\n",
    "x = np.random.normal(0, 5, 100000)\n",
    "pdf_1 = lambda x: norm.pdf(x, 0.5, 5)\n",
    "# mean_estimated = simps(x * pdf_1(x), x) / simps(pdf_1(x), x)\n",
    "# variance_estimated = simps((x - mean_estimated)**2 * pdf_1(x), x)\n",
    "ps.crps_ensemble(3, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c282c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps.crps_gaussian(3, mu=0, sig=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dc7da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_estimated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2586aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_estimated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542c1594",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-30, 30, 10000)\n",
    "pdf_1 = lambda x: norm.pdf(x, 0.5, 5)\n",
    "y = pdf_1(x)\n",
    "\n",
    "# Create the plot\n",
    "plt.plot(x, y)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.title('Normal Distribution with Mean = 0.5 and STD = 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae83b75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "random_numbers = np.random.normal(1, 5, 10000)\n",
    "random_numbers\n",
    "kde = gaussian_kde(random_numbers)\n",
    "a = kde(10)\n",
    "float(0.01581697395876886)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516c339b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "mu1, sigma1 = 0, 2\n",
    "\n",
    "\n",
    "n_samples = 10000\n",
    "X1 = np.random.normal(mu1, sigma1, n_samples)\n",
    "\n",
    "mu2, sigma2 = 8, 2\n",
    "\n",
    "\n",
    "X2 = np.random.normal(mu2, sigma2, n_samples)\n",
    "\n",
    "prob = np.random.choice([0, 1], size=10000)\n",
    "# Draw samples\n",
    "X3 = np.zeros(10000)\n",
    "for i in range(10000):\n",
    "    if prob[i] == 0:\n",
    "        X3[i] = np.random.normal(mu1, sigma1)\n",
    "    else:\n",
    "        X3[i] = np.random.normal(mu2, sigma2)\n",
    "\n",
    "plt.hist(X1, bins=50, density=True, alpha=0.6, color='g')\n",
    "plt.hist(X2, bins=50, density=True, alpha=0.6, color='r')\n",
    "plt.hist(X3, bins=50, density=True, alpha=0.6, color='black')\n",
    "# plt.hist(X4, bins=50, density=True, alpha=0.6, color='purple')\n",
    "plt.title('Histogram of Linear Combination of Normal Distributions')\n",
    "plt.show()\n",
    "\n",
    "number_of_zeros = np.count_nonzero(prob == 0)\n",
    "\n",
    "print(number_of_zeros)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad8919f",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [0.4, 0.6]\n",
    "samples = [X1, X2]\n",
    "X3 = np.sum([s * w for s,w in zip(samples, weights)], axis=0)\n",
    "plt.hist(X3, bins=50, density=True, alpha=0.6, color='g')\n",
    "plt.title('Histogram of Linear Combination of Normal Distributions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2c30ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu1 = 0\n",
    "sigma1 = 1\n",
    "n_samples = 100\n",
    "\n",
    "# Step 1: Generate samples\n",
    "samples = np.random.normal(mu1, sigma1, n_samples)\n",
    "\n",
    "# Step 2: Sort the samples\n",
    "sorted_samples = np.sort(samples)\n",
    "\n",
    "# Step 3: Calculate the CDF\n",
    "cdf = np.arange(1, n_samples + 1) / n_samples\n",
    "\n",
    "# # Step 4: Plot the CDF (optional)\n",
    "# plt.plot(sorted_samples, cdf)\n",
    "# plt.title('Cumulative Distribution Function (CDF)')\n",
    "# plt.xlabel('Value')\n",
    "# plt.ylabel('CDF')\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d4f5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# Parameters for the normal distributions\n",
    "mu1, sigma1 = 0, 1\n",
    "\n",
    "\n",
    "# Generate samples\n",
    "n_samples = 100000\n",
    "percentiles = [2.5, 97.5]\n",
    "X1 = np.percentile(np.random.normal(mu1, sigma1, n_samples), percentiles)\n",
    "print(np.percentile(np.random.normal(mu1, sigma1, n_samples), percentiles)[0])\n",
    "# Plot the result\n",
    "plt.hist(X1, bins=50, density=True, alpha=0.6, color='g')\n",
    "plt.title('Histogram of Linear Combination of Normal Distributions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aa6060",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# Parameters for the normal distributions\n",
    "mu2, sigma2 = 2, 1.5\n",
    "\n",
    "\n",
    "# Generate samples\n",
    "n_samples = 100000\n",
    "X2 = np.random.normal(mu2, sigma2, n_samples)\n",
    "\n",
    "\n",
    "# Plot the result\n",
    "plt.hist(X2, bins=50, density=True, alpha=0.6, color='g')\n",
    "plt.title('Histogram of Linear Combination of Normal Distributions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92e91f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# Parameters for the normal distributions\n",
    "mu1, sigma1 = 0, 1\n",
    "mu2, sigma2 = 2, 1.5\n",
    "\n",
    "# Weights\n",
    "w1, w2 = 0.15, 0.85\n",
    "\n",
    "# Generate samples\n",
    "n_samples = 100000\n",
    "X1 = np.random.normal(mu1, sigma1, n_samples)\n",
    "X2 = np.random.normal(mu2, sigma2, n_samples)\n",
    "\n",
    "# Linear combination\n",
    "Z = w1*X1 + w2*X2\n",
    "\n",
    "# Plot the result\n",
    "plt.hist(Z, bins=50, density=True, alpha=0.6, color='g')\n",
    "plt.title('Histogram of Linear Combination of Normal Distributions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d2c4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = np.mean(Z)\n",
    "var = np.var(Z, ddof=1) # ddof=1: to get an unbiased estimate\n",
    "print(mu, var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64643eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame({'a':[1,2,3], 'b':[4,5,6]})\n",
    "for i, pred in enumerate(a['b']):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615fde30",
   "metadata": {},
   "outputs": [],
   "source": [
    "a['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e106dc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_2d = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "\n",
    "array_2d[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cb0086",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "samples_X = np.random.normal(0, 1, 10000)\n",
    "samples_Y = np.random.normal(1, 2, 10000) \n",
    "\n",
    "def edf(samples, x):\n",
    "    return np.mean(samples <= x)\n",
    "\n",
    "def combined_edf(x, samples_X, samples_Y, w_X, w_Y):\n",
    "    return w_X * edf(samples_X, x) + w_Y * edf(samples_Y, x)\n",
    "\n",
    "# Weights\n",
    "w_X = 0.7\n",
    "w_Y = 0.3\n",
    "\n",
    "# Plotting the combined EDF\n",
    "x_values = np.linspace(min(min(samples_X), min(samples_Y)), max(max(samples_X), max(samples_Y)), 1000)\n",
    "combined_edf_values = [combined_edf(x, samples_X, samples_Y, w_X, w_Y) for x in x_values]\n",
    "\n",
    "samples_X_values = [edf(samples_X, x) for x in x_values]\n",
    "\n",
    "\n",
    "n_samples_X = int(10000 * w_X)  # Number of samples from this set\n",
    "n_samples_Y = int(10000 * w_Y)\n",
    "sampled_X = np.random.choice(samples_X, n_samples_X, replace=True)\n",
    "sampled_Y = np.random.choice(samples_Y, n_samples_Y, replace=True)\n",
    "\n",
    "combined_samples = np.concatenate([sampled_X, sampled_Y])\n",
    "\n",
    "combined_samples_edf_values = [edf(combined_samples, x) for x in x_values]\n",
    "\n",
    "\n",
    "plt.plot(x_values, combined_edf_values, label='Combined EDF', c='blue')\n",
    "plt.plot(x_values, combined_samples_edf_values, label='Combined samples EDF', c='red')\n",
    "plt.plot(x_values, samples_X_values, label='samples_X EDF', c='yellow')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('EDF')\n",
    "plt.title('Combined Empirical Distribution Function')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41aebd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "mu1, sigma1 = 0, 1 \n",
    "mu2, sigma2 = 1, 2  \n",
    "\n",
    "\n",
    "n_samples = 100\n",
    "prob_1 = 0.7\n",
    "prob_2 = 0.3\n",
    "\n",
    "samples_z = np.random.choice([1, 2], size=n_samples, p=[prob_1, prob_2])\n",
    "\n",
    "count_1 = np.sum(samples_z == 1)\n",
    "count_2 = np.sum(samples_z == 2)\n",
    "\n",
    "print(samples_z)\n",
    "combined_samples = np.zeros(n_samples)\n",
    "for i in range(n_samples):\n",
    "    if samples_z[i] == 1:\n",
    "        combined_samples[i] = np.random.normal(mu1, sigma1)\n",
    "    else:\n",
    "        combined_samples[i] = np.random.normal(mu2, sigma2)\n",
    "        \n",
    "# print(combined_samples)\n",
    "print(f'count of componet1 is {count_1}')\n",
    "print(f'count of componet2 is {count_2}')\n",
    "combined_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13268b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### deleteme later\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, mean_absolute_error\n",
    "sb = pd.read_csv('Cardiovascular disease/pred_full/L8_S1.csv', parse_dates = [0], dayfirst = True)\n",
    "sb['epiweek'] = sb['epiweek'].apply(create_epiweek_fromstr)\n",
    "sb = sb.set_index('epiweek') \n",
    "sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e72626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(A, F):\n",
    "    return 1/len(A) * np.sum(np.abs(F - A) / ((np.abs(A) + np.abs(F) + np.finfo(float).eps)/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22eb871",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = list(sb.columns.values)\n",
    "y = sb[['Cardiovascular disease']]\n",
    "model_list.remove('Cardiovascular disease')\n",
    "\n",
    "error_df = pd.DataFrame()\n",
    "#print(model_list)\n",
    "\n",
    "for model in model_list:\n",
    "    model_val = sb[[model]].dropna()\n",
    "    window_start = model_val.index[0]\n",
    "    window_end = model_val.index[-1]\n",
    "    y_val = y.loc[window_start:window_end].copy()\n",
    "    error_df.at[model, 'MSE'] = mean_squared_error(y_val, model_val)\n",
    "    error_df.at[model, 'MAPE'] = mean_absolute_percentage_error(y_val, model_val)\n",
    "    error_df.at[model, 'MAE'] = mean_absolute_error(y_val, model_val)\n",
    "    error_df.at[model, 'SMAPE'] = smape(np.array(y_val), np.array(model_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c588beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbb = np.random.choice(['ww', 'wws', 'fd'], size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b229adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bbb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b96e921",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef684ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "b = np.random.normal(8,2)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c2350d",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = {\n",
    "    'name': 'John',\n",
    "    'city': 'New York'\n",
    "}\n",
    "for item in my_dict.keys():\n",
    "    my_dict[item] =  my_dict[item] + 'k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10bad87",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d13c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "    'data1': np.random.normal(loc=0, scale=1, size=1000),\n",
    "    'data4': np.random.normal(loc=1, scale=1.5, size=1000),\n",
    "    'data3': np.random.normal(loc=2, scale=0.5, size=1000)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef5396e",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_item = list(data_dict.items())[0]\n",
    "for i, j in first_item:\n",
    "    print(i)\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23d6ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3])+0.1\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aac1725",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros_like(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5967f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "a = np.array([[1, 2, 3], \n",
    "                       [4, 5, 6], \n",
    "                       [7, 8, 9]])\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
